{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Platform\n",
    "## Assignment 2: MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By:**  \n",
    "\n",
    "\n",
    "Omer Gralnik, 20637768<br>Yam Daniel, 311515381\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this assignment is to:**\n",
    "- Understand and practice the details of MapReduceEngine\n",
    "\n",
    "**Instructions:**\n",
    "- Students will form teams of two people each, and submit a single homework for each team.\n",
    "- The same score for the homework will be given to each member of your team.\n",
    "- Your solution is in the form of a Jupyter notebook file (with extension ipynb).\n",
    "- Images/Graphs/Tables should be submitted inside the notebook.\n",
    "- The notebook should be runnable and properly documented. \n",
    "- Please answer all the questions and include all your code.\n",
    "- You are expected to submit a clear and pythonic code.\n",
    "- You can change functions signatures/definitions.\n",
    "\n",
    "**Submission:**\n",
    "- Submission of the homework will be done via Moodle by uploading a Jupyter notebook.\n",
    "- The homework needs to be entirely in English.\n",
    "- The deadline for submission is on Moodle.\n",
    "- Late submission won't be allowed.\n",
    "  \n",
    "  \n",
    "- In case of identical code submissions - both groups will get a Zero. \n",
    "- Some groups might be selected randomly to present their code.\n",
    "\n",
    "**Requirements:**  \n",
    "- Python 3.6 should be used.  \n",
    "- You should implement the algorithms by yourself using only basic Python libraries (such as numpy,pandas,etc.)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading:**\n",
    "- Q1 - 5 points - Initial Steps\n",
    "- Q2 - 50 points - MapReduceEngine\n",
    "- Q3 - 30 points - Implement the MapReduce Inverted index of the JSON documents\n",
    "- Q4 - 5 points - Testing Your MapReduce\n",
    "- Q5 - 10 points - Final Thoughts \n",
    "\n",
    "`Total: 100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install names\n",
    "!pip install pytest-shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "# import random\n",
    "import warnings\n",
    "import sqlite3\n",
    "# import csv\n",
    "import names # we chose this package to generate random second names\n",
    "import shutil\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "# ml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hide Warnings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disable Autoscrolling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    \n",
    "return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 1\n",
    "# Initial Steps\n",
    "\n",
    "Write Python code to create 20 different CSV files in this format:  `myCSV[Number].csv`, where each file contains 10 records. \n",
    "\n",
    "The schema is `(‘firstname’,’secondname’,city’)`  \n",
    "\n",
    "Values should be randomly chosen from the lists: \n",
    "- `firstname` : `[John, Dana, Scott, Marc, Steven, Michael, Albert, Johanna]`  \n",
    "- `city` : `[New York, Haifa, München, London, Palo Alto,  Tel Aviv, Kiel, Hamburg]`  \n",
    "- `secondname`: any value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder path in which the data will be saved\n",
    "folder_path = '/Users/omergralnik/Machine Learning _ Data science/First Year/Semester A/Big Data Platforms/Datasets/HW2'\n",
    "\n",
    "# define 'queue' variable which will be used to return the values of the functions in the threads\n",
    "queue = Queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstname = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "city = ['NewYork', 'Haifa', 'Munchen', 'London', 'PaloAlto',  'TelAviv', 'Kiel', 'Hamburg']\n",
    "\n",
    "# using 'names' library we could easily generate random second names\n",
    "secondname = []\n",
    "for i in range(len(firstname)):\n",
    "    random_name = names.get_first_name()\n",
    "    secondname.append(random_name)\n",
    "\n",
    "# let's create the 20 csvs\n",
    "for i in range(1,21):\n",
    "    df = pd.DataFrame(list(zip(firstname, secondname, city)),\n",
    "                columns=['firstname', 'secondname', 'city'])\n",
    "    df.to_csv(f'{folder_path}/myCSV{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use python to Create `mapreducetemp` and `mapreducefinal` folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the folder exist, if so then remove it and create a new one\n",
    "if os.path.exists(f'{folder_path}/mapreducetemp'):\n",
    "    shutil.rmtree(f'{folder_path}/mapreducetemp')\n",
    "os.mkdir(f'{folder_path}/mapreducetemp')\n",
    "\n",
    "if os.path.exists(f'{folder_path}/mapreducefinal'):\n",
    "    shutil.rmtree(f'{folder_path}/mapreducefinal')\n",
    "os.mkdir(f'{folder_path}/mapreducefinal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Question 2\n",
    "## MapReduceEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Python code to create an SQLite database with the following table\n",
    "\n",
    "`TableName: temp_results`   \n",
    "`schema: (key:TEXT,value:TEXT)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(f'{folder_path}/hw2.db')\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS temp_results(\n",
    "                    key text,\n",
    "                    value text)''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Create a Python class** `MapReduceEngine` with method `def execute(input_data, map_function, reduce_function)`, such that:\n",
    "    - `input_data`: is an array of elements\n",
    "    - `map_function`: is a pointer to the Python function that returns a list where each entry of the form (key,value) \n",
    "    - `reduce_function`: is pointer to the Python function that returns a list where each entry of the form (key,value)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Implement** the following functionality in the `execute(...)` function:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. For each key  from the  input_data, start a new Python thread that executes map_function(key) \n",
    "<br><br>\n",
    "2. Each thread will store results of the map_function into mapreducetemp/part-tmp-X.csv where X is a unique number per each thread.\n",
    "<br><br>\n",
    "3. Keep the list of all threads and check whether they are completed.\n",
    "<br><br>\n",
    "4. Once all threads completed, load content of all CSV files into the temp_results table in SQLite.\n",
    "\n",
    "    Remark: Easiest way to loop over all CSV files and load them into Pandas first, then load into SQLite  \n",
    "    `data = pd.read_csv(path to csv)`  \n",
    "    `data.to_sql(‘temp_results’,sql_conn, if_exists=’append’,index=False)`\n",
    "<br><br>\n",
    "\n",
    "5. **Write SQL statement** that generates a sorted list by key of the form `(key, value)` where value is concatenation of ALL values in the value column that match specific key. For example, if table has records\n",
    "<table>\n",
    "    <tbody>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV1.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">Dana</td>\n",
    "                <td style=\"text-align:center\">myCSV5.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV7.csv</td>\n",
    "            </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "    Then SQL statement will return `(‘John’,’myCSV1.csv, myCSV7.csv’)`  \n",
    "    Remark: use GROUP_CONCAT and also GROUP BY ORDER BY\n",
    "<br><br><br>\n",
    "6. **Start a new thread** for each value from the generated list in the previous step, to execute `reduce_function(key,value)` \n",
    "<br>    \n",
    "7. Each thread will store results of reduce_function into `mapreducefinal/part-X-final.csv` file  \n",
    "<br>\n",
    "8. Keep list of all threads and check whether they are completed  \n",
    "<br>\n",
    "9. Once all threads completed, print on the screen `MapReduce Completed` otherwise print `MapReduce Failed` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapReduceEngine():\n",
    "    \n",
    "    def execute(self, input_data, map_function, reduce_function):\n",
    "        \n",
    "        # For each key from the input_data, start a new Python thread that executes map_function(key)\n",
    "        map_threads = []\n",
    "        for key in input_data:\n",
    "            t = threading.Thread(target=map_function, args=[key, ])\n",
    "            t.start()\n",
    "            map_threads.append(t)\n",
    "        \n",
    "        # Each thread will store results of the map_function into mapreducetemp/part-tmp-X.csv \n",
    "        # X is a unique number per each thread\n",
    "        for idx, thread in enumerate(map_threads):\n",
    "            thread.join()\n",
    "            results = queue.get()\n",
    "            df = pd.DataFrame(results, columns=['key', 'value'])\n",
    "            df.to_csv(f'{folder_path}/mapreducetemp/part-tmp-{idx+1}.csv', index=False)\n",
    "         \n",
    "        # Keep the list of all threads and check whether they are completed\n",
    "        for t in map_threads:\n",
    "            if not t.is_alive():\n",
    "                # get results from thread\n",
    "                t.handled = True\n",
    "        map_threads_validate = [t for t in map_threads if t.handled] # the list of all valid threads\n",
    "        \n",
    "        # check if whether all of the threads are completed\n",
    "        if len(map_threads) == len(map_threads_validate):\n",
    "            print('Map treads completed succesfully')\n",
    "            completed = True\n",
    "        else:\n",
    "            completed = False\n",
    "            print('Map treads failed!!')\n",
    "            return\n",
    "        \n",
    "        # Once all threads completed, load content of all CSV files into the temp_results table in SQLite\n",
    "        if completed:\n",
    "            for file in os.listdir(f'{folder_path}/mapreducetemp'):\n",
    "                df = pd.read_csv(f'{folder_path}/mapreducetemp/{file}')\n",
    "                df.to_sql('temp_results',connection, if_exists='append',index=False)\n",
    "            \n",
    "            # Write SQL statement that generates a sorted list by key of the form (key, value) \n",
    "            # where value is concatenation of ALL values in the value column that match specific key\n",
    "            cursor.execute('''SELECT Key, GROUP_CONCAT(value) FROM temp_results GROUP BY Key ORDER BY Key''')\n",
    "            key_value_list = cursor.fetchall()\n",
    "            \n",
    "        # Start a new thread for each value from the generated list in the previous step, to execute reduce_function(key,value)\n",
    "        reduce_threads = []\n",
    "        for value in key_value_list:\n",
    "            t = threading.Thread(target=reduce_function, args=[value[0], value[1],])\n",
    "            t.start()\n",
    "            reduce_threads.append(t)\n",
    "        \n",
    "        # Each thread will store results of reduce_function into mapreducefinal/part-X-final.csv file\n",
    "        for idx, thread in enumerate(reduce_threads):\n",
    "            thread.join()\n",
    "            results = queue.get()\n",
    "            df = pd.DataFrame(results[1:], columns=[results[0]])\n",
    "            df.to_csv(f'{folder_path}/mapreducefinal/part-{idx+1}-final.csv', index=False)\n",
    "        \n",
    "        # Keep list of all threads and check whether they are completed\n",
    "        for t in reduce_threads:\n",
    "            if not t.is_alive():\n",
    "                # get results from thread\n",
    "                t.handled = True\n",
    "        reduce_threads_validate = [t for t in reduce_threads if t.handled] # the list of all valid threads\n",
    "        \n",
    "        # Once all threads completed, print on the screen MapReduce Completed otherwise print MapReduce Failed\n",
    "        if len(reduce_threads) == len(reduce_threads_validate):\n",
    "            print('Reduce treads completed succesfully')\n",
    "            return '\\nMapReduce Completed'\n",
    "        else:\n",
    "            return '\\nMapReduce Failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 3\n",
    "## Implement the MapReduce Inverted index of the JSON documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `inverted_map(document_name)` which reads the CSV document from the local disc and return a list that contains entries of the form (key_value, document name).\n",
    "\n",
    "For example, if myCSV4.csv document has values like:  \n",
    "`{‘firstname’:’John’,‘secondname’:’Rambo’,‘city’:’Palo Alto’}`\n",
    "\n",
    "Then `inverted_map(‘myCSV4.csv’)` function will return a list:  \n",
    "`[(‘firstname_John’,’ myCSV4.csv’),(‘secondname_Rambo’,’ myCSV4.csv’), (‘city_Palo Alto’,’ myCSV4.csv’)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ We will use a decorator which its goal is to return the values of the functions that will be executed by the threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the decorator\n",
    "def ResultsQueue(func):\n",
    "    def wrapper(*args):\n",
    "        queue.put(func(*args))\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@ResultsQueue\n",
    "def inverted_map(document_name: str):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function reads the CSV document from the local disc \n",
    "    and return a list that contains entries of the form (key_value, document name)\n",
    "    \"\"\"\n",
    "    # read the csv and create an empty list\n",
    "    data = pd.read_csv(f'{folder_path}/{document_name}')\n",
    "    inverted_list = []\n",
    "    \n",
    "    # convert each row in the csv to a dictionary and add it to the list\n",
    "    for i in range(len(data)):\n",
    "        row_dict = dict(data.iloc[i, :]) # {firstname: John, secondname: Bon, city: TelAviv}\n",
    "        for key, value in row_dict.items():\n",
    "            inverted_list.append((f'{key}_{value}',document_name)) # (firstname_John, secondname_Bon, city_TelAviv)\n",
    "    return inverted_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a reduce function `inverted_reduce(value, documents)`, where the field “documents” contains a list of all CSV documents per given value.   \n",
    "This list might have duplicates.   \n",
    "Reduce function will return new list without duplicates.\n",
    "\n",
    "For example,  \n",
    "calling the function `inverted_reduce(‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’)`   \n",
    "will return a list `[‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ResultsQueue\n",
    "def inverted_reduce(value, documents):\n",
    "    \"\"\"\n",
    "    The field “documents” contains a list of all CSV documents per given value.\n",
    "    This list might have duplicates.\n",
    "    The function will return new list without duplicates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert the long string of csvs to a list\n",
    "    documents = documents.split(',') \n",
    "    \n",
    "    # return a list without duplicates\n",
    "    return [value]+list(dict.fromkeys(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 4\n",
    "## Testing Your MapReduce\n",
    "\n",
    "**Create Python list** `input_data` : `[‘myCSV1.csv’,.. ,‘myCSV20.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [csv for csv in os.listdir(folder_path) if 'csv' in csv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit MapReduce as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapreduce = MapReduceEngine()\n",
    "status = mapreduce.execute(input_data, inverted_map, inverted_reduce)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that `MapReduce Completed` should be printed and `mapreducefinal` folder should contain the result files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use python to delete all temporary data from mapreducetemp folder and delete SQLite database:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the folder exists, delete it recursively\n",
    "if os.path.exists(f'{folder_path}/mapreducetemp'):\n",
    "    shutil.rmtree(f'{folder_path}/mapreducetemp')\n",
    "\n",
    "# try to close the connection\n",
    "try:\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "# if the exception is 'ProgrammingError' then the connection is already close\n",
    "# print it\n",
    "except sqlite3.ProgrammingError:\n",
    "    print('Connection is already closed!')\n",
    "\n",
    "# if the database exists - remove it\n",
    "if os.path.exists(f'{folder_path}/hw2.db'):\n",
    "    os.remove(f'{folder_path}/hw2.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 5\n",
    "# Final Thoughts\n",
    "\n",
    "The phase where `MapReduceEngine` reads all temporary files generated by maps and sort them to provide each reducer a specific key is called the **shuffle step**.\n",
    "\n",
    "Please explain **clearly** what would be the main problem of MapReduce when processing Big Data, if there is no shuffle step at all, meaning reducers will directly read responses from the mappers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map and reduce processes are independent of each other, and the connection between them is the shuffle step. If there is no shuffle step at all, the reducers would not have any input from every mapper, that is because the shuffle step transfers the map output to the reducer as input.<br>The absence of the shuffle step could also cause a situation where 2 maps outputs that are supposed to get the same reducer, will get a different reducer and the process will fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "Good Luck :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
