{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Platform\n",
    "## Assignment 3: ServerLess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By:**  \n",
    "\n",
    "Yam Daniel, 311515381  \n",
    "Omer Gralnik, 206337768\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this assignment is to:**\n",
    "- Understand and practice the details of Serverless\n",
    "\n",
    "**Instructions:**\n",
    "- Students will form teams of two people each, and submit a single homework for each team.\n",
    "- The same score for the homework will be given to each member of your team.\n",
    "- Your solution is in the form of a Jupyter notebook file (with extension ipynb).\n",
    "- Images/Graphs/Tables should be submitted inside the notebook.\n",
    "- The notebook should be runnable and properly documented. \n",
    "- Please answer all the questions and include all your code.\n",
    "- You are expected to submit a clear and pythonic code.\n",
    "- You can change functions signatures/definitions.\n",
    "\n",
    "**Submission:**\n",
    "- Submission of the homework will be done via Moodle by uploading (not Zip):\n",
    "    - Jupyter Notebook\n",
    "    - 2 Log files\n",
    "    - Additional local scripts\n",
    "- The homework needs to be entirely in English.\n",
    "- The deadline for submission is on Moodle.\n",
    "- Late submission won't be allowed.\n",
    "\n",
    "  \n",
    "- In case of identical code submissions - both groups will get a Zero. \n",
    "- Some groups might be selected randomly to present their code.\n",
    "\n",
    "**Requirements:**  \n",
    "- Python 3.6 should be used.  \n",
    "- You should implement the algorithms by yourself using only basic Python libraries (such as numpy,pandas,etc.)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading:**\n",
    "- Q0 - 10 points - Setup\n",
    "- Q1 - 40 points - Serverless MapReduceEngine\n",
    "- Q2 - 20 points - MapReduce job to calculate inverted index\n",
    "- Q3 - 30 points - Shuffle\n",
    "\n",
    "`Total: 100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 0\n",
    "## Setup\n",
    "\n",
    "1. Navigate to IBM Cloud and open a trial account. No need to provide a credit card\n",
    "2. Choose IBM Cloud Object Storage service from the catalog\n",
    "3. Create a new bucket in IBM Cloud Object Storage\n",
    "4. Create credentials for the bucket with HMAC (access key and secret key)\n",
    "5. Choose IBM Cloud Functions service from the catalog and create a service\n",
    "\n",
    "\n",
    "#### Lithops setup\n",
    "1. By using “git” tool, install master branch of the Lithops project from\n",
    "https://github.com/lithops-cloud/lithops\n",
    "2. Follow Lithops documentation and configure Lithops against IBM Cloud Functions and IBM Cloud Object Storage\n",
    "3. Configure Lithops log level to be in DEBUG mode\n",
    "4. Run Hello World example by using Futures API and verify all is working properly.\n",
    "\n",
    "\n",
    "#### IBM Cloud Object Storage setup\n",
    "1. Upload all the input CSV files that you used in homework 2 into the bucket you created in IBM Cloud Object Storage\n",
    "\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lithops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import lithops\n",
    "import shutil\n",
    "import names\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "! lithops test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder path in which the data will be saved\n",
    "folder_path = './outputs'\n",
    "if os.path.isdir('./outputs'):\n",
    "    shutil.rmtree('./outputs')\n",
    "os.mkdir('./outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstname = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "city = ['NewYork', 'Haifa', 'Munchen', 'London', 'PaloAlto',  'TelAviv', 'Kiel', 'Hamburg']\n",
    "\n",
    "# using 'names' library we could easily generate random second names\n",
    "secondname = []\n",
    "for i in range(len(firstname)):\n",
    "    random_name = names.get_first_name()\n",
    "    secondname.append(random_name)\n",
    "\n",
    "# let's create the 20 csvs\n",
    "for i in range(1,21):\n",
    "    firstname_col = random.choices(firstname, k=10)\n",
    "    secondname_col = random.choices(secondname, k=10)\n",
    "    city_col = random.choices(city, k=10)\n",
    "    df = pd.DataFrame(list(zip(firstname_col, secondname_col, city_col)),\n",
    "                columns=['firstname', 'secondname', 'city'])\n",
    "    df.to_csv(f'{folder_path}/myCSV{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the folder exist, if so then remove it and create a new one\n",
    "if os.path.exists(f'{folder_path}/mapreducetemp'):\n",
    "    shutil.rmtree(f'{folder_path}/mapreducetemp')\n",
    "os.mkdir(f'{folder_path}/mapreducetemp')\n",
    "\n",
    "if os.path.exists(f'{folder_path}/mapreducefinal'):\n",
    "    shutil.rmtree(f'{folder_path}/mapreducefinal')\n",
    "os.mkdir(f'{folder_path}/mapreducefinal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(f'{folder_path}/hw3.db')\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS temp_results(\n",
    "                    key text,\n",
    "                    value text)''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "## Serverless MapReduceEngine\n",
    "\n",
    "Modify MapReduceEngine from homework 2 into the MapReduceServerlessEngine where map and reduce tasks executed as a serverless actions, instead of local threads. In particular:\n",
    "1. Deploy all map tasks as a serverless actions by using Lithops against IBM Cloud Functions.\n",
    "2. Collect results from all map tasks and store them in the same SQLite as you used in MapReduceEngine and use the same code for the sort and shuffle phase.\n",
    "3. Deploy reduce tasks by using Lithops against IBM Cloud Functions. Instead of persisting results from reduce tasks, return results back to the MapReduceServerlessEngine and proceed with the same workflow as in MapReduceEngine\n",
    "4. Return results of reduce tasks to the user\n",
    "\n",
    "**Please attach:**  \n",
    "Text file with all log messages Lithops printed to console during the execution. Make\n",
    "sure log level is set to DEBUG mode.\n",
    "\n",
    "#### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapReduceServerlessEngine():\n",
    "    \n",
    "    def execute(self, input_data, map_function, reduce_function):\n",
    "        \n",
    "        with lithops.ServerlessExecutor(log_level='DEBUG') as fexec:\n",
    "\n",
    "            for idx, key in enumerate(input_data):\n",
    "\n",
    "                key = f\"{folder_path}/{key}\"\n",
    "                f = fexec.call_async(map_function, (key, idx))\n",
    "                f.result()\n",
    "\n",
    "            for file in os.listdir(f'{folder_path}/mapreducetemp'):\n",
    "\n",
    "                df = pd.read_csv(f'{folder_path}/mapreducetemp/{file}')\n",
    "                df.to_sql('temp_results',connection, if_exists='append',index=False)\n",
    "            \n",
    "            cursor.execute('''SELECT key, GROUP_CONCAT(value) FROM temp_results GROUP BY key ORDER BY key''')\n",
    "            key_value_list = cursor.fetchall()\n",
    "\n",
    "            for idx, value in enumerate(key_value_list):\n",
    "                f = fexec.call_async(reduce_function, (value[0], value[1], idx))\n",
    "                f.result()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_map(document_path: str, idx: int):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function reads the CSV document from the local disc \n",
    "    and return a list that contains entries of the form (key_value, document name)\n",
    "    \"\"\"\n",
    "    # read the csv and create an empty list\n",
    "    document_name = document_path.split('/')[-1]\n",
    "    data = pd.read_csv(document_path)\n",
    "\n",
    "    inverted_list = []\n",
    "    \n",
    "    # convert each row in the csv to a dictionary and add it to the list\n",
    "    for i in range(len(data)):\n",
    "        row_dict = dict(data.iloc[i, :]) # {firstname: John, secondname: Bon, city: TelAviv}\n",
    "        for key, value in row_dict.items():\n",
    "            inverted_list.append((f'{key}_{value}',document_name)) # (firstname_John, secondname_Bon, city_TelAviv)\n",
    "    \n",
    "    df = pd.DataFrame(inverted_list, columns=['key', 'value'])\n",
    "    df.to_csv(f'{folder_path}/mapreducetemp/part-tmp-{idx+1}.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_reduce(value, documents, idx):\n",
    "    \"\"\"\n",
    "    The field “documents” contains a list of all CSV documents per given value.\n",
    "    This list might have duplicates.\n",
    "    The function will return new list without duplicates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert the long string of csvs to a list\n",
    "    documents = documents.split(',') \n",
    "    results = [value]+list(dict.fromkeys(documents))\n",
    "    df = pd.DataFrame(results[1:], columns=[results[0]])\n",
    "    df.to_csv(f'{folder_path}/mapreducefinal/part-{idx+1}-final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "## Submit MapReduce job to calculate inverted index\n",
    "1. Use input_data: `cos://bucket/<path to CSV data>`\n",
    "2. Submit MapReduce job with reduce and map functions as you used in homework 2, as follows\n",
    "\n",
    "    `mapreduce = MapReduceServerlessEngine()`  \n",
    "    `results = mapreduce.execute(input_data, inverted_map, inverted_index)`   \n",
    "    `print(results)`\n",
    "\n",
    "**Please attach:**  \n",
    "Text file with all log messages Lithops printed to console during the execution. Make\n",
    "sure log level is set to DEBUG mode.\n",
    "\n",
    "#### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [csv for csv in os.listdir(folder_path) if 'csv' in csv]\n",
    "mapreduce = MapReduceServerlessEngine()\n",
    "mapreduce.execute(input_data, inverted_map, inverted_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the folder exists, delete it recursively\n",
    "if os.path.exists(f'{folder_path}/mapreducetemp'):\n",
    "    shutil.rmtree(f'{folder_path}/mapreducetemp')\n",
    "\n",
    "# try to close the connection\n",
    "try:\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "# if the exception is 'ProgrammingError' then the connection is already close\n",
    "# print it\n",
    "except sqlite3.ProgrammingError:\n",
    "    print('Connection is already closed!')\n",
    "\n",
    "# if the database exists - remove it\n",
    "if os.path.exists(f'{folder_path}/hw2.db'):\n",
    "    os.remove(f'{folder_path}/hw2.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "## Shuffle\n",
    "\n",
    "MapReduceServerlessEngine deploys both map and reduce tasks as serverless invocations.   \n",
    "However, once map stage completed, the result are transferred from the map tasks to the SQLite database located on the client machine (laptop in your case), then performed local shuffle and then invoked reduce tasks passing them relevant parameters.\n",
    "\n",
    "(To support your answers, feel free to use examples, Images, etc.)\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Explain why this approach is not efficient and what are cons and pros of such architecture in general. In broader scope you may assume that MapReduceServerlessEngine executed in some powerful machine and not just laptop.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach has it's Strengths and weeknesses like any other matter, we will name some of the cons and prons for ServerlessEngine architecture we see as most important.\n",
    "<br><ins>cons:</ins>\n",
    "1. most of the implementations still require at least one long-running server in a job execution\n",
    "2. Some implementations of simple serverless MapReduce that cannot express complex data processing jobs.\n",
    "3. some implementations are not open-source, not maintained or have an extremely complicated setup.\n",
    "\n",
    "<br><ins>pros:</ins>\n",
    "1. The Cloud provider dynamically allocates resources to the function invocations and fine-grained billing is introduced depending on the execution time and allocated memory, resulting in pay-as-you-go solution for the user.\n",
    "2. This method leverage the high elasticity of serverless computing to efficiently support jobs whose resource requirements vary significantly during their execution.\n",
    "3. Serverless computing is simple and powerful way to run embarrassingly parallel computations or harness proprietary services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "**2. Suggest how can you improve shuffle so intermediate data will not be downloaded to the client at all and shuffle performed in the cloud as well. Explain pros and cons of the approaches you suggest.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will present our thoughts regarding shuffling data using cloud services rather than shuffling data using a server, and include our conclusion.\n",
    "\n",
    "Shuffling data on a severless engine costs very low when using slow storage systems (as low as 0.005$ per 1000 requests), but when sorting great amount of data, it will sum up to very expensive costs. Therefore, using server less services such as s3 for example will be very expensive Considering the running time.\n",
    "\n",
    "Using shuffling on a severless engine with fast storage systems like Memcached or Redis, will be much more efficient and overcome the performance of the slow storage, but these systems are typically much more expensive than large-scale blob storage systems.\n",
    "\n",
    "\n",
    "Our conclusion, Given the cost-performance trade-off between slow (S3) and fast ( Redis) storage, we recommend combining these two types of storage systems, so we can achieve a cost-performance sweet spot in a severless deployment that is comparable, and sometimes superior to cluster-based deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "**3. Can you make serverless shuffle?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\<your answer here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "Good Luck :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
